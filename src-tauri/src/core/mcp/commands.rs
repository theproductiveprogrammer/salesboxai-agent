use rmcp::model::{CallToolRequestParam, CallToolResult, Content};
use serde_json::{Map, Value};
use tauri::{AppHandle, Emitter, Runtime, State};
use tokio::time::timeout;
use tokio::sync::oneshot;

use super::{
    constants::{DEFAULT_MCP_CONFIG, MCP_TOOL_CALL_TIMEOUT},
    helpers::{restart_active_mcp_servers, start_mcp_server_with_restart, stop_mcp_servers, schedule_mcp_reconnect},
};
use crate::core::{app::commands::get_jan_data_folder_path, state::AppState};
use crate::core::{
    mcp::models::ToolWithServer,
    state::{RunningServiceEnum, SharedMcpServers},
};
use std::fs;

#[tauri::command]
pub async fn activate_mcp_server<R: Runtime>(
    app: tauri::AppHandle<R>,
    state: State<'_, AppState>,
    name: String,
    config: Value,
) -> Result<(), String> {
    let servers: SharedMcpServers = state.mcp_servers.clone();

    // Use the modified start_mcp_server_with_restart that returns first attempt result
    start_mcp_server_with_restart(app, servers, name, config, Some(3)).await
}

#[tauri::command]
pub async fn deactivate_mcp_server(state: State<'_, AppState>, name: String) -> Result<(), String> {
    log::info!("Deactivating MCP server: {}", name);

    // First, mark server as manually deactivated to prevent restart
    // Remove from active servers list to prevent restart
    {
        let mut active_servers = state.mcp_active_servers.lock().await;
        active_servers.remove(&name);
        log::info!("Removed MCP server {} from active servers list", name);
    }

    // Mark as not successfully connected to prevent restart logic
    {
        let mut connected = state.mcp_successfully_connected.lock().await;
        connected.insert(name.clone(), false);
        log::info!("Marked MCP server {} as not successfully connected", name);
    }

    // Reset restart count
    {
        let mut counts = state.mcp_restart_counts.lock().await;
        counts.remove(&name);
        log::info!("Reset restart count for MCP server {}", name);
    }

    // Now remove and stop the server
    let servers = state.mcp_servers.clone();
    let mut servers_map = servers.lock().await;

    let service = servers_map
        .remove(&name)
        .ok_or_else(|| format!("Server {} not found", name))?;

    // Release the lock before calling cancel
    drop(servers_map);

    match service {
        RunningServiceEnum::NoInit(service) => {
            log::info!("Stopping server {name}...");
            service.cancel().await.map_err(|e| e.to_string())?;
        }
        RunningServiceEnum::WithInit(service) => {
            log::info!("Stopping server {name} with initialization...");
            service.cancel().await.map_err(|e| e.to_string())?;
        }
    }
    log::info!("Server {name} stopped successfully and marked as deactivated.");
    Ok(())
}

#[tauri::command]
pub async fn restart_mcp_servers(app: AppHandle, state: State<'_, AppState>) -> Result<(), String> {
    let servers = state.mcp_servers.clone();
    // Stop the servers
    stop_mcp_servers(state.mcp_servers.clone()).await?;

    // Restart only previously active servers (like cortex)
    restart_active_mcp_servers(&app, servers).await?;

    app.emit("mcp-update", "MCP servers updated")
        .map_err(|e| format!("Failed to emit event: {}", e))?;

    Ok(())
}

/*
/// Reinitialize MCP servers (download from remote + start builtin)
/// This is called after login when the endpoint is changed
#[tauri::command]
pub async fn reinitialize_mcp_servers(app: AppHandle, state: State<'_, AppState>) -> Result<(), String> {
    use crate::core::setup::install_mcp_from_remote;
    use super::helpers::start_builtin_salesbox_mcp;

    // Acquire global mutex to prevent concurrent reinitialization calls
    let lock = REINIT_LOCK.get_or_init(|| TokioMutex::new(()));
    let _guard = lock.lock().await;

    let servers = state.mcp_servers.clone();

    log::info!("Reinitializing MCP servers after login/endpoint change");

    // Mark server as not connected to prevent monitoring tasks from restarting it
    {
        let mut connected = state.mcp_successfully_connected.lock().await;
        connected.insert("salesboxai-builtin".to_string(), false);
        log::info!("Marked SalesboxAI MCP server as not connected to prevent restart loops");
    }

    // Reset restart count to prevent accumulation
    {
        let mut counts = state.mcp_restart_counts.lock().await;
        counts.remove("salesboxai-builtin");
    }

    // Stop existing builtin server first to prevent orphaned processes
    {
        let mut servers_map = servers.lock().await;
        if let Some(service) = servers_map.remove("salesboxai-builtin") {
            log::info!("Stopping existing SalesboxAI MCP server before reinitialize");
            match service {
                RunningServiceEnum::NoInit(s) => { let _ = s.cancel().await; }
                RunningServiceEnum::WithInit(s) => { let _ = s.cancel().await; }
            }
        }
    }

    // Small delay to allow monitoring tasks to exit after seeing the server removed
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;

    // Download/update MCP services from remote (uses current endpoint from store)
    if let Err(e) = install_mcp_from_remote(app.clone()).await {
        log::warn!("Failed to download MCP from remote: {}", e);
    }

    // Start the built-in SalesboxAI MCP server (if credentials are configured)
    if let Err(e) = start_builtin_salesbox_mcp(&app, servers.clone()).await {
        log::warn!("SalesboxAI builtin MCP server not started: {}", e);
    }

    app.emit("mcp-update", "MCP servers updated")
        .map_err(|e| format!("Failed to emit event: {}", e))?;

    Ok(())
}
*/

/// Reset MCP restart count for a specific server (like cortex reset)
#[tauri::command]
pub async fn reset_mcp_restart_count(
    state: State<'_, AppState>,
    server_name: String,
) -> Result<(), String> {
    let mut counts = state.mcp_restart_counts.lock().await;

    let count = match counts.get_mut(&server_name) {
        Some(count) => count,
        None => return Ok(()), // Server not found, nothing to reset
    };

    let old_count = *count;
    *count = 0;
    log::info!(
        "MCP server {} restart count reset from {} to 0.",
        server_name,
        old_count
    );
    Ok(())
}

#[tauri::command]
pub async fn get_connected_servers(
    _app: AppHandle,
    state: State<'_, AppState>,
) -> Result<Vec<String>, String> {
    let servers = state.mcp_servers.clone();
    let servers_map = servers.lock().await;
    Ok(servers_map.keys().cloned().collect())
}

/// Retrieves all available tools from all MCP servers with server information
///
/// # Arguments
/// * `state` - Application state containing MCP server connections
///
/// # Returns
/// * `Result<Vec<Tool>, String>` - A vector of all tools if successful, or an error message if failed
///
/// This function:
/// 1. Locks the MCP servers mutex to access server connections
/// 2. Iterates through all connected servers
/// 3. Gets the list of tools from each server
/// 4. Associates each tool with its parent server name
/// 5. Combines all tools into a single vector
/// 6. Returns the combined list of all available tools with server information
#[tauri::command]
pub async fn get_tools(state: State<'_, AppState>) -> Result<Vec<ToolWithServer>, String> {
    let servers = state.mcp_servers.lock().await;
    let mut all_tools: Vec<ToolWithServer> = Vec::new();

    for (server_name, service) in servers.iter() {
        // List tools with timeout
        let tools_future = service.list_all_tools();
        let tools = match timeout(MCP_TOOL_CALL_TIMEOUT, tools_future).await {
            Ok(result) => result.map_err(|e| e.to_string())?,
            Err(_) => {
                log::warn!(
                    "Listing tools timed out after {} seconds",
                    MCP_TOOL_CALL_TIMEOUT.as_secs()
                );
                continue; // Skip this server and continue with others
            }
        };

        for tool in tools {
            all_tools.push(ToolWithServer {
                name: tool.name.to_string(),
                description: tool.description.as_ref().map(|d| d.to_string()),
                input_schema: serde_json::Value::Object((*tool.input_schema).clone()),
                server: server_name.clone(),
            });
        }
    }

    Ok(all_tools)
}

/// Calls a tool on an MCP server by name with optional arguments
///
/// # Arguments
/// * `state` - Application state containing MCP server connections
/// * `tool_name` - Name of the tool to call
/// * `arguments` - Optional map of argument names to values
/// * `cancellation_token` - Optional token to allow cancellation from JS side
///
/// # Returns
/// * `Result<CallToolResult, String>` - Result of the tool call if successful, or error message if failed
///
/// This function:
/// 1. Locks the MCP servers mutex to access server connections
/// 2. Searches through all servers for one containing the named tool
/// 3. When found, calls the tool on that server with the provided arguments
/// 4. If the call fails with a transport error, triggers reconnection and retries once
/// 5. Supports cancellation via cancellation_token
/// 6. Returns error if no server has the requested tool
#[tauri::command]
pub async fn call_tool(
    app: AppHandle,
    state: State<'_, AppState>,
    tool_name: String,
    arguments: Option<Map<String, Value>>,
    cancellation_token: Option<String>,
) -> Result<CallToolResult, String> {
    // Use a loop to handle retry without recursion (Rust async recursion requires boxing)
    let max_attempts = 2; // Initial attempt + 1 retry
    let mut attempt = 0;
    let mut last_error: Option<String> = None;
    let mut reconnect_server: Option<String> = None;

    while attempt < max_attempts {
        attempt += 1;

        // If we need to reconnect from a previous failed attempt
        if let Some(server_name) = reconnect_server.take() {
            log::info!("Attempting reconnection for server {} before retry", server_name);
            if let Err(reconnect_err) = schedule_mcp_reconnect(&app, &state, &server_name).await {
                log::error!("Failed to reconnect MCP server {}: {}", server_name, reconnect_err);
                return Err(format!(
                    "Error calling tool {}: {} (reconnection failed: {})",
                    tool_name,
                    last_error.unwrap_or_default(),
                    reconnect_err
                ));
            }
            log::info!("Reconnected successfully, retrying tool call {}", tool_name);
        }

        // Set up cancellation if token is provided (only on first attempt)
        let cancel_rx = if attempt == 1 {
            if let Some(token) = &cancellation_token {
                let (cancel_tx, cancel_rx) = oneshot::channel::<()>();
                let mut cancellations = state.tool_call_cancellations.lock().await;
                cancellations.insert(token.clone(), cancel_tx);
                Some(cancel_rx)
            } else {
                None
            }
        } else {
            None
        };

        let servers = state.mcp_servers.lock().await;
        log::info!("Attempt {}: servers in map: {:?}", attempt, servers.keys().collect::<Vec<_>>());

        // Iterate through servers and find the first one that contains the tool
        for (server_name, service) in servers.iter() {
            log::info!("Checking server {} for tool {}", server_name, tool_name);

            // Check cache first for this server's tools (permanent cache, never expires)
            let cached_tools = {
                let cache = state.mcp_tool_cache.lock().await;
                cache.get(server_name).cloned()
            };

            let tools = if let Some(cached) = cached_tools {
                log::debug!("Using cached tool list for server {} ({} tools)", server_name, cached.len());
                cached
            } else {
                // First time: fetch and cache permanently
                match service.list_all_tools().await {
                    Ok(tools) => {
                        let mut cache = state.mcp_tool_cache.lock().await;
                        cache.insert(server_name.clone(), tools.clone());
                        log::info!("Cached {} tools for server {} (permanent)", tools.len(), server_name);
                        tools
                    }
                    Err(e) => {
                        let err_str = e.to_string();
                        log::warn!("Failed to list tools from server {}: {}", server_name, err_str);
                        // If this looks like a transport error and we can retry, schedule reconnect
                        if attempt < max_attempts && is_transport_error(&err_str) {
                            log::info!("Transport error detected for server {}, will attempt reconnection", server_name);
                            reconnect_server = Some(server_name.clone());
                            last_error = Some(err_str);
                            drop(servers); // Release lock before reconnect loop
                            break; // Break inner loop to trigger reconnect in outer loop
                        }
                        continue; // Skip this server if we can't list tools
                    }
                }
            };

            if !tools.iter().any(|t| t.name == tool_name) {
                continue; // Tool not found in this server, try next
            }

            log::info!("Found tool {} in server {}", tool_name, server_name);

            // Clone for potential retry
            let arguments_clone = arguments.clone();
            let server_name_for_retry = server_name.clone();

            // Call the tool with timeout and cancellation support
            let tool_call = service.call_tool(CallToolRequestParam {
                name: tool_name.clone().into(),
                arguments: arguments_clone,
            });

            // Race between timeout, tool call, and cancellation
            let result = if let Some(cancel_rx) = cancel_rx {
                tokio::select! {
                    result = timeout(MCP_TOOL_CALL_TIMEOUT, tool_call) => {
                        match result {
                            Ok(call_result) => call_result.map_err(|e| e.to_string()),
                            Err(_) => Err(format!(
                                "Tool call '{}' timed out after {} seconds",
                                tool_name,
                                MCP_TOOL_CALL_TIMEOUT.as_secs()
                            )),
                        }
                    }
                    _ = cancel_rx => {
                        Err(format!("Tool call '{}' was cancelled", tool_name))
                    }
                }
            } else {
                match timeout(MCP_TOOL_CALL_TIMEOUT, tool_call).await {
                    Ok(call_result) => call_result.map_err(|e| e.to_string()),
                    Err(_) => Err(format!(
                        "Tool call '{}' timed out after {} seconds",
                        tool_name,
                        MCP_TOOL_CALL_TIMEOUT.as_secs()
                    )),
                }
            };

            // Clean up cancellation token
            if let Some(token) = &cancellation_token {
                let mut cancellations = state.tool_call_cancellations.lock().await;
                cancellations.remove(token);
            }

            // Check if result is a transport error that warrants retry
            if let Err(ref err) = result {
                if attempt < max_attempts && is_transport_error(err) {
                    log::info!("Transport error on tool call, will attempt reconnection for server {}", server_name_for_retry);
                    reconnect_server = Some(server_name_for_retry);
                    last_error = Some(err.clone());
                    drop(servers); // Release lock before reconnect loop
                    break; // Break inner loop to trigger reconnect in outer loop
                }
            }

            return result;
        }

        // If no reconnect scheduled (tool not found via MCP), try HTTP fallback
        if reconnect_server.is_none() {
            // Try HTTP fallback for built-in tools before giving up
            log::info!("Tool {} not found via MCP, attempting HTTP fallback", tool_name);
            match http_fallback_tool_call(&tool_name, &arguments, &state).await {
                Ok(result) => {
                    log::info!("HTTP fallback succeeded for tool {}", tool_name);
                    return Ok(result);
                }
                Err(fallback_err) => {
                    log::warn!("HTTP fallback also failed for tool {}: {}", tool_name, fallback_err);
                    // Continue to error below
                }
            }
            break;
        }
    }

    Err(format!("Tool {} not found", tool_name))
}

/// Check if an error string indicates a transport/connection error
fn is_transport_error(error: &str) -> bool {
    let transport_indicators = [
        "Transport",
        "transport",
        "connection",
        "Connection",
        "HTTP",
        "status client error",
        "status server error",
        "network",
        "Network",
        "refused",
        "reset",
        "closed",
    ];
    transport_indicators.iter().any(|indicator| error.contains(indicator))
}

/// Maps MCP tool names to their HTTP REST API endpoints in salesboxai-core
fn get_http_fallback_endpoint(tool_name: &str) -> Option<&'static str> {
    match tool_name {
        "prospect_lead" => Some("/mcp/prospect-lead"),
        "send_email" => Some("/mcp/send-email"),
        "get_lead_info" => Some("/mcp/lead-info"),
        "find_leads" => Some("/mcp/discover-leads"),
        "list_jobs" => Some("/mcp/job-list"),
        "get_job_info" => Some("/mcp/job-status"),
        "delete_job" => Some("/mcp/job-delete"),
        "cancel_job" => Some("/mcp/job-cancel"),
        "retry_job" => Some("/mcp/job-retry"),
        "send_linkedin_message" => Some("/mcp/send-linkedin-message"),
        "comment_on_linkedin_post" => Some("/mcp/comment-on-post"),
        "react_to_linkedin_post" => Some("/mcp/react-to-post"),
        _ => None,
    }
}

/// Transforms MCP tool arguments to the REST API format expected by salesboxai-core.
/// MCP tools use camelCase args like `linkedinUrl`, but REST endpoints expect
/// ChatCtxDTO format with `agentContext.lead_linkedin` etc.
fn transform_mcp_args_to_rest(tool_name: &str, args: &Option<Map<String, Value>>) -> Value {
    let args = match args {
        Some(a) => a,
        None => return serde_json::json!({}),
    };

    match tool_name {
        // Tools that expect ChatCtxDTO with agentContext
        "get_lead_info" | "prospect_lead" => {
            let mut agent_context = serde_json::Map::new();

            // Map MCP arg names to agentContext field names
            if let Some(v) = args.get("linkedinUrl") {
                agent_context.insert("lead_linkedin".to_string(), v.clone());
            }
            if let Some(v) = args.get("leadId") {
                agent_context.insert("lead_id".to_string(), v.clone());
            }
            if let Some(v) = args.get("leadName") {
                agent_context.insert("lead_name".to_string(), v.clone());
            }
            if let Some(v) = args.get("leadEmail") {
                agent_context.insert("lead_email".to_string(), v.clone());
            }
            if let Some(v) = args.get("leadTitle") {
                agent_context.insert("lead_title".to_string(), v.clone());
            }
            if let Some(v) = args.get("leadCompany") {
                agent_context.insert("lead_company".to_string(), v.clone());
            }

            serde_json::json!({
                "agentContext": agent_context
            })
        }
        // Tools that can pass args directly (already match REST format)
        _ => Value::Object(args.clone()),
    }
}

/// Attempts to call a tool via direct HTTP REST API as a fallback when MCP fails.
/// This ensures tools remain available even when MCP connection is unstable.
async fn http_fallback_tool_call(
    tool_name: &str,
    arguments: &Option<Map<String, Value>>,
    state: &AppState,
) -> Result<CallToolResult, String> {
    let endpoint = get_http_fallback_endpoint(tool_name)
        .ok_or_else(|| format!("No HTTP fallback available for tool {}", tool_name))?;

    // Get the API endpoint and auth header from the active servers config (salesboxai-builtin)
    // Clone the values we need before releasing the lock
    let (full_url, auth_header) = {
        let active_servers = state.mcp_active_servers.lock().await;
        let builtin_config = active_servers.get("salesboxai-builtin")
            .ok_or_else(|| "salesboxai-builtin server not configured".to_string())?;

        // Extract the base URL from the MCP config
        let mcp_url = builtin_config.get("url")
            .and_then(|v| v.as_str())
            .ok_or_else(|| "MCP URL not found in config".to_string())?;

        // Convert MCP URL to REST API URL (remove /mcp suffix if present)
        let base_url = mcp_url.trim_end_matches("/mcp");
        let full_url = format!("{}{}", base_url, endpoint);

        // Get auth token from headers
        let auth_header = builtin_config.get("headers")
            .and_then(|h| h.get("Authorization"))
            .and_then(|v| v.as_str())
            .ok_or_else(|| "Authorization header not found".to_string())?
            .to_string();

        (full_url, auth_header)
    }; // Lock released here

    log::info!("HTTP fallback: calling {} for tool {}", full_url, tool_name);

    // Transform MCP arguments to REST API format
    let body = transform_mcp_args_to_rest(tool_name, arguments);
    log::debug!("HTTP fallback body: {:?}", body);

    // Make HTTP POST request
    let client = reqwest::Client::new();
    let response = client
        .post(&full_url)
        .header("Authorization", auth_header)
        .header("Content-Type", "application/json")
        .json(&body)
        .send()
        .await
        .map_err(|e| format!("HTTP fallback request failed: {}", e))?;

    let status = response.status();
    let response_text = response.text().await
        .map_err(|e| format!("Failed to read HTTP fallback response: {}", e))?;

    if !status.is_success() {
        log::error!("HTTP fallback failed with status {}: {}", status, response_text);
        return Err(format!("HTTP fallback failed with status {}: {}", status, response_text));
    }

    log::info!("HTTP fallback succeeded for tool {}", tool_name);

    Ok(CallToolResult {
        content: vec![Content::text(response_text)],
        is_error: Some(false),
        structured_content: None,
    })
}

/// Cancels a running tool call by its cancellation token
///
/// # Arguments
/// * `state` - Application state containing cancellation tokens
/// * `cancellation_token` - Token identifying the tool call to cancel
///
/// # Returns
/// * `Result<(), String>` - Success if token found and cancelled, error otherwise
#[tauri::command]
pub async fn cancel_tool_call(
    state: State<'_, AppState>,
    cancellation_token: String,
) -> Result<(), String> {
    let mut cancellations = state.tool_call_cancellations.lock().await;
    
    if let Some(cancel_tx) = cancellations.remove(&cancellation_token) {
        // Send cancellation signal - ignore if receiver is already dropped
        let _ = cancel_tx.send(());
        println!("Tool call with token {} cancelled", cancellation_token);
        Ok(())
    } else {
        Err(format!("Cancellation token {} not found", cancellation_token))
    }
}

#[tauri::command]
pub async fn get_mcp_configs(app: AppHandle) -> Result<String, String> {
    let mut path = get_jan_data_folder_path(app);
    path.push("mcp_config.json");
    log::info!("read mcp configs, path: {:?}", path);

    // Create default empty config if file doesn't exist
    if !path.exists() {
        log::info!("mcp_config.json not found, creating default empty config");
        fs::write(&path, DEFAULT_MCP_CONFIG)
            .map_err(|e| format!("Failed to create default MCP config: {}", e))?;
    }

    fs::read_to_string(path).map_err(|e| e.to_string())
}

#[tauri::command]
pub async fn save_mcp_configs(app: AppHandle, configs: String) -> Result<(), String> {
    let mut path = get_jan_data_folder_path(app);
    path.push("mcp_config.json");
    log::info!("save mcp configs, path: {:?}", path);

    fs::write(path, configs).map_err(|e| e.to_string())
}
